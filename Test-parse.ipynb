{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90a42bf1-2a28-4cb1-80ac-07314234db57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Notebook for EDA on WikiData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa133b8e-198f-4823-a715-ec13ca6aefd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Config for blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a108f71-3baf-4587-be5c-f28c669f3977",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = 'wikidatasubset'\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"databricks-secret-keys\", key=\"blob\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b9f217-7724-487f-bfdd-fe7745c9866a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define schema for table Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96139a5c-b096-4e5f-b488-543742c04b10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "item_schema = StructType([\n",
    "    StructField(\"pageid\", LongType(), True),\n",
    "    StructField(\"ns\", LongType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"lastrevid\", LongType(), True),\n",
    "    StructField(\"modified\", TimestampType(), True),\n",
    "    StructField(\"id\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82d4345-d203-4735-964b-20ff22322419",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'testdataset/latest-first-1gb.json'\n",
    "\n",
    "df_item = spark.read.option(\"mode\", \"DROPMALFORMED\").schema(item_schema).json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a62834b-2a8e-46bc-9132-f303087d7fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_item.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bbc1fbc-9491-4fb3-8244-a7c9209c1b36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define schema for table Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea073e1-9b90-4073-af79-2fe433229ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, StructField, StringType\n",
    "\n",
    "labels_struct = StructType([\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "labels_map = MapType(StringType(), labels_struct)\n",
    "\n",
    "labels_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"labels\", labels_map, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45ead25-3bb1-4bf3-b346-c902ed63a0aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'testdataset/latest-first-1gb.json'\n",
    "\n",
    "df_label = spark.read.json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}', schema=labels_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d3b6b6-f3c9-4a46-9e51-e3569f0390eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df_label_final = (\n",
    "    df_label.where(col(\"type\") == 'item').select(\"id\", explode(\"labels\").alias(\"language\", \"struct\"))\n",
    "    .select(\"id\", \"language\", col(\"struct.value\").alias(\"label\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc088bcf-b9f9-4c8d-b75c-f873f0f0d4e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_label_final.write.format('parquet').save(f'abfss://{container}@{storage_account}.dfs.core.windows.net/preprocessed/aliases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44ed79c7-df77-4a7e-ae38-3c852505f237",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define schema for table Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5344a26-f245-4270-8a98-ea3a5a3e32b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, StructField, StringType\n",
    "\n",
    "descriptions_struct = StructType([\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "descriptions_map = MapType(StringType(), descriptions_struct)\n",
    "\n",
    "descriptions_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"descriptions\", descriptions_map, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861fb320-d418-4733-bb4f-c10c2e36924c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'testdataset/latest-first-1gb.json'\n",
    "\n",
    "df_descriptions = spark.read.json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}', schema=descriptions_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35b96b5-ae12-4080-b704-eff91d87137a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_descriptions_final = (\n",
    "    df_descriptions.select(\"id\", explode(\"descriptions\").alias(\"language\", \"struct\"))\n",
    "    .select(\"id\", \"language\", col(\"struct.value\").alias(\"description\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461dc525-98e2-4498-b8d3-4c59dd3ca687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_descriptions_final.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15798922-d403-488f-b86a-fb187849cc45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define schema for table Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bf8743-4034-477e-bb72-2a4397d5be6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, StructField, StringType\n",
    "\n",
    "alias_struct = StructType([\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "aliases_map = MapType(StringType(), ArrayType(alias_struct))\n",
    "\n",
    "aliases_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"aliases\", aliases_map, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02832eb7-6153-4bdb-ab3a-c2695f5b8e24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'testdataset/latest-first-1gb.json'\n",
    "\n",
    "df_aliases = spark.read.json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}', schema=aliases_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e95965-f64a-488a-87cc-ce0b0d49a0f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df_aliases_final = (\n",
    "    df_aliases.select(\"id\", explode(\"aliases\").alias(\"language\", \"array\"))\n",
    "    .select(\"id\", \"language\", explode(\"array.value\").alias(\"alias\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8b4f73-9a3b-4d2a-bdc8-6214dcbdc579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_aliases_final.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "671aa0e2-1305-4b55-94b6-c2140c2c03ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define schema for table Sitelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8f8325-12f5-414d-bd2e-a1e2c5085c9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, StructField, StringType\n",
    "\n",
    "sitelinks_struct = StructType([\n",
    "    StructField(\"site\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"badges\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "sitelinks_map = MapType(StringType(), sitelinks_struct)\n",
    "\n",
    "sitelinks_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"sitelinks\", sitelinks_map, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a3723a-5e1d-404a-8515-7b53ece6a735",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'testdataset/latest-first-1gb.json'\n",
    "\n",
    "df_sitelinks = spark.read.json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}', schema=sitelinks_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ec0d47-df9f-47ee-9d9a-f29dd6c0ec3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, explode_outer\n",
    "\n",
    "df_sitelinks_final = (\n",
    "    df_sitelinks.select(\"id\", explode(\"sitelinks\").alias(\"wiki\", \"struct\"))\n",
    "    .select(\"id\", \"wiki\", col(\"struct.title\").alias(\"title\"), explode_outer(\"struct.badges\").alias(\"badges\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a2448d-97ec-4e8d-8efe-35673f2fe8d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sitelinks_final.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2a6e8d-2480-4109-b77f-a4cac893e765",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define schema for table Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8120464a-5e97-4cad-b0bf-78d566f0a243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, StructField, StringType\n",
    "\n",
    "value_struct = StructType([\n",
    "    StructField(\"id\", StringType(), True)\n",
    "])\n",
    "\n",
    "datavalue_struct = StructType([\n",
    "    StructField(\"value\", value_struct, True)\n",
    "])\n",
    "\n",
    "mainsnak_struct = StructType([\n",
    "    StructField(\"datavalue\", datavalue_struct, True)\n",
    "])\n",
    "\n",
    "claims_struct = StructType([\n",
    "    StructField(\"mainsnak\", mainsnak_struct, True),\n",
    "])\n",
    "\n",
    "claims_map = MapType(StringType(), ArrayType(claims_struct))\n",
    "\n",
    "claims_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"claims\", claims_map, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab0d655-138a-4999-987f-3140f3ac72ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'testdataset/latest-first-1gb.json'\n",
    "\n",
    "df_claims = spark.read.json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}', schema=claims_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507aba1d-7aee-484f-adf4-271b57d417d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, explode_outer\n",
    "\n",
    "df_claims_final = (\n",
    "    df_claims.select(\"id\", explode(\"claims\").alias(\"claim\", \"claim_datas\"))\n",
    "    .select(\"id\", \"claim\", explode(\"claim_datas\").alias(\"claim_data\"))\n",
    "    .select(col(\"id\").alias(\"src\"), col(\"claim_data.mainsnak.datavalue.value.id\").alias(\"dst\"), col(\"claim\").alias(\"property\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01506be-8121-4295-a4a1-f5729d1d1f54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_claims_final.head(100)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Test-parse",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
