{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2219913b-11eb-46f6-b9bd-491376b35458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = 'wikidatasubset'\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"databricks-secret-keys\", key=\"blob\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3033589-8d7b-42b9-a066-b3ddcec85474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, StructField, StringType\n",
    "\n",
    "labels_struct = StructType([\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "labels_map = MapType(StringType(), labels_struct)\n",
    "\n",
    "labels_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"labels\", labels_map, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213027c5-40d5-4cf8-9fbd-eb2842c0a31d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'wikidatawiki/entities/latest-all.json'\n",
    "\n",
    "df_label = spark.read.json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}', schema=labels_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe64b4c7-814e-469a-8c57-7b0800689eac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df_label_final = (\n",
    "    df_label.where(col(\"type\") == 'item').select(\"id\", explode(\"labels\").alias(\"language\", \"struct\"))\n",
    "    .select(\"id\", \"language\", col(\"struct.value\").alias(\"label\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fea9b9-7de2-41d3-a343-e523ad2c16fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_label_final.write.format('parquet').save(f'abfss://{container}@{storage_account}.dfs.core.windows.net/preprocessed/aliases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29db8ac-c496-4af8-b4f4-d616e4bead29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, StructField, StringType\n",
    "\n",
    "value_struct = StructType([\n",
    "    StructField(\"id\", StringType(), True)\n",
    "])\n",
    "\n",
    "datavalue_struct = StructType([\n",
    "    StructField(\"value\", value_struct, True)\n",
    "])\n",
    "\n",
    "mainsnak_struct = StructType([\n",
    "    StructField(\"datavalue\", datavalue_struct, True)\n",
    "])\n",
    "\n",
    "claims_struct = StructType([\n",
    "    StructField(\"mainsnak\", mainsnak_struct, True),\n",
    "])\n",
    "\n",
    "claims_map = MapType(StringType(), ArrayType(claims_struct))\n",
    "\n",
    "claims_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"claims\", claims_map, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16710569-285e-4b4e-9b55-2032f9301f94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = 'init'\n",
    "path = 'wikidatawiki/entities/latest-all.json'\n",
    "\n",
    "df_claims = spark.read.json(f'abfss://{container}@{storage_account}.dfs.core.windows.net/{path}', schema=claims_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47303f9e-ec5e-47c8-a806-33e4d0a4fabd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, explode_outer\n",
    "\n",
    "df_claims_final = (\n",
    "    df_claims.select(\"id\", explode(\"claims\").alias(\"claim\", \"claim_datas\"))\n",
    "    .select(\"id\", \"claim\", explode(\"claim_datas\").alias(\"claim_data\"))\n",
    "    .select(col(\"id\").alias(\"src\"), col(\"claim_data.mainsnak.datavalue.value.id\").alias(\"dst\"), col(\"claim\").alias(\"property\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994a20d3-410f-4559-9579-b091eaf974df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_claims_final.write.format('parquet').save(f'abfss://{container}@{storage_account}.dfs.core.windows.net/preprocessed/claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f41a83-95f0-4529-aa57-e8b5af3e55b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_claims_final_read = spark.read.parquet(f'abfss://{container}@{storage_account}.dfs.core.windows.net/preprocessed/claims')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Parse-json",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
